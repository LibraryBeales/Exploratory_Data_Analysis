{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email [rdb104@case.edu](mailto:rdb104@case.edu).<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in Python\n",
    "\n",
    "**Description:** This lesson introduces the basic data import and simple assessment processes using the `pandas` library for Python.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 15 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** csv files\n",
    "\n",
    "**Data Format:** `csv`, `py` \n",
    "\n",
    "**Libraries Used:** `pandas` \n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to your first web scraping project.  This project will use the `requests` package to introduce the basic web scraping workflow.\n",
    "\n",
    "We will be using [2014 Adult Census Data](https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv) as our example for this tutorial.  The csv file is taken from this [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/adult-census-income?resource=download).  \n",
    "\n",
    "In this project you will:\n",
    "\n",
    "1. Use the `read_csv` function in pandas to import a csv file.\n",
    "2. Discover the size of the dataset.\n",
    "3. Discover the types of data in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultra Quick Jupyter Notebook Tips\n",
    "\n",
    "These Jupyter Notebooks have markdown cells, which you just read, and code cells, which you can run and edit.  \n",
    "\n",
    "Code cells appear in the light grey box.  You can click on the text in the code cell to edit it.  You can run a code cell by clicking the Run button at the top of the page (pictured) or by clicking Shift + Enter after clicking on the cell.\n",
    "\n",
    "![The Run Button](img/runcellbutton.png) \n",
    "\n",
    "Finally, all the code cells in a notebook must be run in order.  Make sure you start at the top of each lesson and run each cell in order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "\n",
    "We're going to be using the `pandas` package for most of our exploratory data analysis work.  Before we can begin using a Python package, we have to import it.  We will import `pandas as pd` so that when we have to call on fuctions from the package inour code we can juse use the shorter `pd` insteard of typing out `pandas` every time.  This may not seem like a big difference, but it is a common practice so it is a good idea to be aware of it in case you see code in the future using this sort ofshorthand for package names.  \n",
    "\n",
    "Run the cell below to import the `pandas` package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #https://pandas.pydata.org/docs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the `pandas` package has been imported we can use the various excellent functions for importing data that are built into the package. Our data is a csv file.  CSV is a very common data format that is widely supported. It is easy for humans to read, making data analysis and maintenance much simpler.  It can also store large amounts of data in relatively small file sizes.  You can learn more about csv files [here](https://en.wikipedia.org/wiki/Comma-separated_values) if you are curious.\n",
    "\n",
    "Our data is stored here: (https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv)  This is a set of data pulled from the 2014 census by Ronny Kohavi and Barry Becker and posted on [Kaggle](https://www.kaggle.com/datasets/uciml/adult-census-income/data).  \n",
    "\n",
    "To import our csv file we are going to use the `pd.read_csv` function that is built into `pandas`.  You can learn more about this function [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).  But right now, all we need to do is call the function and give it a the required parameter, a location of a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
      "0   90         ?   77053       HS-grad              9        Widowed   \n",
      "1   82   Private  132870       HS-grad              9        Widowed   \n",
      "2   66         ?  186061  Some-college             10        Widowed   \n",
      "3   54   Private  140359       7th-8th              4       Divorced   \n",
      "4   41   Private  264663  Some-college             10      Separated   \n",
      "\n",
      "          occupation   relationship   race     sex  capital.gain  \\\n",
      "0                  ?  Not-in-family  White  Female             0   \n",
      "1    Exec-managerial  Not-in-family  White  Female             0   \n",
      "2                  ?      Unmarried  Black  Female             0   \n",
      "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
      "4     Prof-specialty      Own-child  White  Female             0   \n",
      "\n",
      "   capital.loss  hours.per.week native.country income  \n",
      "0          4356              40  United-States  <=50K  \n",
      "1          4356              18  United-States  <=50K  \n",
      "2          4356              40  United-States  <=50K  \n",
      "3          3900              40  United-States  <=50K  \n",
      "4          3900              40  United-States  <=50K  \n"
     ]
    }
   ],
   "source": [
    "# URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python sent a request to the URL we specified, https://books.toscrape.com in this case.  The response we got back is the first piece of information you need when web scraping.  Could we connect?!\n",
    "\n",
    "The 200 is one of what are called **http response status codes** and it means our attempt to connect was a success.  Excellent!\n",
    "\n",
    "A code in the 400s or 500s would mean there was an error connecting to the server.  \n",
    "\n",
    "If you want to learn more about the status codes, you can check out the developer documentation from Mozilla here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the content of a response.\n",
    "\n",
    "The status code is only one piece of information in the response object.  Now that we've made a successful connection to a website, we can start to look at the other information in the response object returned by our `get` request.  \n",
    "\n",
    "The first thing we'll need to do is store the response object in a variable using the `=` so we can look at the content without repeatedly sending requests to the web server.\n",
    "\n",
    "The code below is the same as our first `get` request, but it is stored in the variable `response`.  You could name this variable anything you wanted.  In this example we chose `results` as a shorthand for response object.  If you change the variable name here, you'll have to change it in all the following code examples as well.  So, just leave it as `results` for now.  \n",
    "\n",
    "When you're ready, run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get('https://books.toscrape.com/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice this time, you didn't get an http status code as a response.  When you store a response object in a variable, python won't display any information from the response object unless you use a command to ask it to show a piece of the response stored there. \n",
    "\n",
    "If we want to check the http status code again, we can call the variable `response` and the `status_code` attribute of the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.status_code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object has many other attibutes as well:\n",
    "* .headers - access header information about the server that sent the response\n",
    "* .text - access the response body as text for text-based responses, such as html, json, yaml, etc.\n",
    "* .content - access the response body as bytes for nontext requests, such as images, spreadsheets, zipfiles, etc.\n",
    "* .encoding - shows what text encoding `requests` is using in the `.text` attribute\n",
    "* .url - shows the url that was used in the request, can be useful when encoding urls with various parameters\n",
    "* .json - builtin in json decoder for scraping json files\n",
    "\n",
    "Try viewwing these different attributes by editing the code cell below and running it again to see what you get.  All the attributes follow the same format, `results` where we stored our response object, a period, the the attribute, (i.e., `text`, `headers`, `url`)\n",
    "\n",
    "For example, `results.text`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Response Object \n",
    "\n",
    "Now that we have a response object and taken a quick look at its attributes, we need to save it to a file so that we can come back to it in the future and examine it.  We could scrape the web again each time we want to do some analysis, but that would be a waste of computing resources, and the content on the web is not static.  Social media posts are deleted.  Items are removed from shops.  The format of websites changes making your web scraper obsolete.  So it is best to scrape once, and save the data locally. Saving it also allows us to easily share the data we are using with others.\n",
    "\n",
    "We are going to use a `with` statement to simplify the process of saving the response object.  The `with` statement in Python is used when you want to execute several operations as a group.  We will open the file, write the response object to it, and close the file and all these operations will happen within the `with` statement. You can learn more about `with` statments and writing to files here: https://www.freecodecamp.org/news/with-open-in-python-with-statement-syntax-example/\n",
    "\n",
    "You can see the structure of the `with` statement in the code cell below.  \n",
    "\n",
    "`with` begins the with statement.  \n",
    "`open` tells the computer to open a file using the filename you specify, in this case, `scrape.txt`, and the `w` indicates we want to write to the file.  \n",
    "`as` creates a variable that contains the file information, in this case, `file`.  \n",
    "`file.write` will save whatever attributes of the response object you include in the parentheses, (i.e., `results.text`, `results.headers`, `results.url`)\n",
    "\n",
    "The filename, `scrape.txt`, and the attribute can be changed to create different files with diffferent content.  \n",
    "\n",
    "When you run the code below you will see the file appear in the list of directories/files on the left.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape.txt','w') as file:\n",
    "    file.write(results.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the filename below to `scrapeheaders.txt` and the attribute below to `response.headers` and run the code cell again to create another file.  Did you get a differnt file with different content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape.txt','w') as file:\n",
    "    file.write(results.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
