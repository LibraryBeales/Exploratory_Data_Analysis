{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email [rdb104@case.edu](mailto:rdb104@case.edu).<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in Python\n",
    "\n",
    "**Description:** This lesson introduces the basic data import and simple assessment processes using the `pandas` library for Python.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 15 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** csv files\n",
    "\n",
    "**Data Format:** `csv`, `py` \n",
    "\n",
    "**Libraries Used:** `pandas` \n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to your first EDA lesson.  This project will use the `pandas` package to introduce the basic EDA workflow.\n",
    "\n",
    "We will be using [2014 Adult Census Data](https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv) as our example for this tutorial.  The csv file is taken from this [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/adult-census-income?resource=download).  \n",
    "\n",
    "In this project you will:\n",
    "1. Learn how to import the necessary python libraries.\n",
    "2. Use the `read_csv` function in pandas to import a csv file.\n",
    "3. Discover the size of the dataset.\n",
    "4. Discover the types of data in the dataset. \n",
    "5. Learn how to modify data types to improve performance, accuracy and scalability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultra Quick Jupyter Notebook Tips\n",
    "\n",
    "These Jupyter Notebooks have markdown cells, which you just read, and code cells, which you can run and edit.  \n",
    "\n",
    "Code cells appear in the light grey box.  You can click on the text in the code cell to edit it.  You can run a code cell by clicking the Run button at the top of the page (pictured) or by clicking Shift + Enter after clicking on the cell.\n",
    "\n",
    "![The Run Button](img/runcellbutton.png) \n",
    "\n",
    "Finally, all the code cells in a notebook must be run in order.  Make sure you start at the top of each lesson and run each cell in order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "\n",
    "We'll be using the `pandas` package for most of our exploratory data analysis. Before we can use a Python package, we need to import it. We’ll import `pandas as pd`, allowing us to use the shorter `pd` instead of typing `pandas` each time we call its functions. While this may seem like a small change, it's a common practice in Python, so it's helpful to be familiar with it.\n",
    "\n",
    "Run the cell below to import the pandas package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #https://pandas.pydata.org/docs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `pandas` package imported, we can use its powerful functions to bring data into our workspace. Our data is in a CSV file—a widely supported format that's easy for humans to read and ideal for data analysis and maintenance. CSV files also allow for efficient storage of large datasets. If you're interested, you can read more about CSV files [here](https://en.wikipedia.org/wiki/Comma-separated_values) if you are curious.\n",
    "\n",
    "Our data can be found at this URL: (https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv)  This dataset, originally from the 2014 U.S. Census, was compiled by Ronny Kohavi and Barry Becker and is hosted on [Kaggle](https://www.kaggle.com/datasets/uciml/adult-census-income/data).  \n",
    "\n",
    "To import the CSV file, we'll use the `pd.read_csv` function. This function simply requires the file's location, which in our case is the URL above. You can also use `read_csv` to load data from a file path on your computer. For more details on this function, see the official documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "adultCensus = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the functions `head()`, `info()`, and `describe()` are frequently used for preliminary data exploration and analysis. \n",
    "\n",
    "Here's a quick breakdown of the usage and purpose of each one:\n",
    "\n",
    "1. head()\n",
    "\n",
    "    Purpose: Displays the first few rows of a DataFrame or Series.\n",
    "    Usage: DataFrame.head(n)\n",
    "    Parameter: n (optional) - the number of rows to display (default is 5).\n",
    "\n",
    "2. info()\n",
    "\n",
    "    Purpose: Provides a concise summary of a DataFrame, including the index dtype, column dtypes, non-null values, and memory usage.  Helps in understanding the structure of the DataFrame, checking for null values, and confirming data types.\n",
    "    Usage: DataFrame.info()\n",
    "\n",
    "3. describe()\n",
    "\n",
    "    Purpose: Generates descriptive statistics of numerical columns in a DataFrame, including count, mean, standard deviation, min, 25th percentile, median (50%), 75th percentile, and max.  Useful for quickly getting insights into the distribution and summary statistics of the data.\n",
    "    Usage: DataFrame.describe()\n",
    "\n",
    "Try running all three functions in the code cells below.  What did you learn about our dataset after using these functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultCensus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Display the first few rows of the DataFrame\n",
    "adultCensus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Display the first few rows of the DataFrame\n",
    "adultCensus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Data\n",
    "\n",
    "After running the `info()` function, you'll see that our dataset has 15 columns and 32,561 rows.\n",
    "\n",
    "The output of `info()` also shows the data type (Dtype) of each column. Understanding these data types is essential for data cleaning, transformation, and optimizing operations in pandas. A pandas DataFrame can contain various data types in its columns, and knowing these types helps with effective data manipulation.\n",
    "\n",
    "While working with smaller datasets or practicing exploratory data analysis (EDA) and visualization, optimization may not be crucial. However, converting columns to appropriate data types is a best practice that offers several advantages.  For example, Many `pandas` operations are optimized for certain data types. Categorical data types are more efficient for string-based columns with limited, repeated values, improving speed and reducing memory usage. Memory efficiency, improved performance, and data integrity are some other benefits.  \n",
    "\n",
    "Using appropriate data types is a habit that can lead to better performance when working with large or complex datasets, so adopting these habits now will not only make your code more useful when you move on to larger projects, but it will also be helpful to others who may be working on more complex projects.\n",
    "\n",
    "So to get started with data types, let's run the code below to get another look at the data types in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data type of each column in the adultCensus dataframe \n",
    "adultCensus.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have only two data types, `object`, a data type for strings of characters or mixed data, and `int64`, a numeric data type for whole numbers.  \n",
    "\n",
    "You can see above that we have several columns with the `object` data type.  This data type is commonly used as a catch-all and seeing so many columns with this data type should raise a red flag.  \n",
    "\n",
    "If we're going to change data types, we should know what our options are.  Here’s an overview of the main data types you might encounter in a DataFrame.  There are a few more, but these are the most common.\n",
    "\n",
    "1. Numeric Types\n",
    "\n",
    "    int64 / int32: Integer data types, used for columns with whole numbers.\n",
    "    Example: 1, 10, -42\n",
    "    float64 / float32: Floating-point data types, used for columns with decimal numbers.\n",
    "    Example: 3.14, -0.001, 2.5\n",
    "\n",
    "2. Object Type\n",
    "\n",
    "    object: A catch-all data type, typically used for columns that contain strings or mixed data types.\n",
    "    Example: 'apple', '123 Main St', 'true'\n",
    "    Note: This type can include any Python object and is commonly used when data contains text or heterogeneous types. It is generally less efficient than specific types like string.\n",
    "\n",
    "3. String Type\n",
    "\n",
    "    string: Explicit type for columns with text data. While object can hold strings, using string ensures more consistent handling.\n",
    "    Example: 'Hello, world!', 'data science'\n",
    "    Note: This type is useful for text processing and can be more optimized than using object for string data.\n",
    "\n",
    "4. Boolean Type\n",
    "\n",
    "    bool: Represents columns with True or False values.\n",
    "    Example: True, False, True\n",
    "    Use Case: Commonly used in conditions, filters, and logical operations.\n",
    "\n",
    "5. Datetime Type\n",
    "\n",
    "    datetime64: Used for columns containing date and time information.\n",
    "    Example: 2023-11-04 15:45:00, 2024-01-01\n",
    "    Use Case: Facilitates operations like date arithmetic, filtering by dates, and time-series analysis.\n",
    "\n",
    "6. Timedelta Type\n",
    "\n",
    "    timedelta64: Represents differences or durations between dates and times.\n",
    "    Example: 3 days 00:00:00, -1 days +23:00:00\n",
    "    Use Case: Useful for measuring time spans or differences between date columns.\n",
    "\n",
    "7. Category Type\n",
    "\n",
    "    category: Represents data that is limited to a fixed number of distinct values, similar to enumerations.\n",
    "    Example: ['Red', 'Blue', 'Green']\n",
    "    Use Case: Optimizes memory usage and performance when working with columns that have repeated discrete values, like status labels or ratings.\n",
    "    Advantage: Reduces memory usage compared to object and speeds up operations involving comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration, doing some counting...\n",
    "\n",
    "So. We've got many columns in our data frame with the data type `object.`  Many of these seem like the data would be better classified as the `category` data type.  `Category` data has a fixed number of possible values.   `marital.status`, `race`, `sex` seem like they would be good candidates for the `category` data type.  `marital.status` might contain values like \"Married,\" \"Single,\" \"Divorced,\" etc. `race` could include values like \"White,\" \"Black,\" \"Asian,\" etc. `sex` typically has values like \"Male\" and \"Female.\"\n",
    "\n",
    "One way to check if these fields are a good candidate for the `category` data type is to check for the number of unique values.  `Pandas` has a method specifically for this aspect of EDA, `nunique()`.  \n",
    "\n",
    "Run the code cell below to get a count of unique values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultCensus.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we were right about the `marital.status`, `race`, and `sex` columns in our dataframe.  Out of ~32k rows, there are less that 10 unique values in all three columns.  These are defintely good candidates for the `category` data type.  \n",
    "\n",
    "However! We've also discovered that `workclass`, `education`, `occupation`, `relationship`, and `income`have very few unique values as well.  `income` is the one that is really surprising here.  Do you agree?  Why are there only 2 unique values?\n",
    "\n",
    "Let's use the `head()` method from `pandas` again to take another look at the data.  By default, it shows the top 5 rows, but you can specify the number of rows you’d like to see by passing an integer as an argument to head().  The code cell below has the argument 30 in the parentheses, so we will see the firsrt 30 rows.  \n",
    "\n",
    "Run the code cell below.  What do you see in the `income` field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultCensus.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `income` field is just an indicator that tells us whether or not the iondividual makes more than or less than fifty thousand dollars.  So this field is also a perfect candidate for the `category` datatype. \n",
    "\n",
    "Before we convert these fields of our dataframe to the new datatype, let's review why this is important.\n",
    "\n",
    "Memory Efficiency: The category type stores unique values only once and references them with an integer, which is more efficient than storing strings. This reduces memory usage, which is especially beneficial for large datasets with repeated values in these columns.\n",
    "\n",
    "Performance Improvement: Operations like grouping, filtering, or comparisons can be faster with categorical data because pandas can optimize them based on the limited number of unique values.\n",
    "\n",
    "Logical Representation: By converting these columns to category types, you indicate that these fields represent qualitative, not quantitative, information. This makes the code more readable and the data structure clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['marital.status'] = df['marital.status'].astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object\n",
    "\n",
    "Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape.txt','w') as file:\n",
    "    file.write(results.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape.txt','w') as file:\n",
    "    file.write(results.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
