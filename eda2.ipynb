{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email [rdb104@case.edu](mailto:rdb104@case.edu).<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in Python\n",
    "\n",
    "**Description:** This lesson introduces the concept of data ypes in a `pandas` dataframe and demostrates processes for modifying data types to improve performance, readbility, and effciency.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 30 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** csv files\n",
    "\n",
    "**Data Format:** `csv`, `py` \n",
    "\n",
    "**Libraries Used:** `pandas` \n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to your first EDA lesson.  This project will use the `pandas` package to introduce the basic EDA workflow.\n",
    "\n",
    "We will be using [2014 Adult Census Data](https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv) as our example for this tutorial.  The csv file is taken from this [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/adult-census-income?resource=download).  \n",
    "\n",
    "In this project you will:\n",
    "1. Learn how to view the data types in a `pandas` dataframe.\n",
    "2. Learn some new methods for exploring the `pandas` dataframe.\n",
    "3. Learn to modify the data type of a single column.\n",
    "4. Learn how to modify data types of multiple columns using a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultra Quick Jupyter Notebook Tips\n",
    "\n",
    "These Jupyter Notebooks have markdown cells, which you just read, and code cells, which you can run and edit.  \n",
    "\n",
    "Code cells appear in the light grey box.  You can click on the text in the code cell to edit it.  You can run a code cell by clicking the Run button at the top of the page (pictured) or by clicking Shift + Enter after clicking on the cell.\n",
    "\n",
    "![The Run Button](img/runcellbutton.png) \n",
    "\n",
    "Finally, all the code cells in a notebook must be run in order.  Make sure you start at the top of each lesson and run each cell in order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pandas\n",
    "\n",
    "We'll be using the `pandas` package again.  We’ll import `pandas as pd`, allowing us to use the shorter `pd` instead of typing `pandas` each time we call its functions. While this may seem like a small change, it's a common practice in Python, so it's helpful to be familiar with it.\n",
    "\n",
    "Run the cell below to import the pandas package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #https://pandas.pydata.org/docs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to import the csv file with our data again, as this is a new jupyter notebook.\n",
    "\n",
    "To import the CSV file, we'll use the `pd.read_csv` function. This function simply requires the file's location, which in our case is the URL above. You can also use `read_csv` to load data from a file path on your computer. For more details on this function, see the official documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "adultCensus = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've successfully imported the csv, you can check to see the data looks the way you expect it to using `adultCensus.info()`.  We should have 32561 rows and 15 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Display a concise summary of the DataFrame, showing information about each column, including data types, non-null counts, and memory usage. \n",
    "adultCensus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Data\n",
    "\n",
    "In addition to the amount of data, the output of `info()` also shows the data type (Dtype) of each column. Understanding these data types is essential for data cleaning, transformation, and optimizing operations in pandas. A pandas DataFrame can contain various data types in its columns, and knowing these types helps with effective data manipulation.\n",
    "\n",
    "While working with smaller datasets or practicing exploratory data analysis (EDA) and visualization, optimization may not be crucial. However, converting columns to appropriate data types is a best practice that offers several advantages.  For example, Many `pandas` operations are optimized for certain data types. Categorical data types are more efficient for string-based columns with limited, repeated values, improving speed and reducing memory usage. Improved performance and data integrity are some other benefits.  \n",
    "\n",
    "Using appropriate data types is a habit that can lead to better performance when working with large or complex datasets, so adopting these habits now will not only make your code more useful when you move on to larger projects, but it will also be helpful to others who may be working on more complex projects.\n",
    "\n",
    "So to get started with data types, let's run the code below to get another look at the data types in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data type of each column in the adultCensus dataframe \n",
    "adultCensus.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have only two data types, `object`, a data type for strings of characters or mixed data, and `int64`, a numeric data type for whole numbers.  \n",
    "\n",
    "You can see above that we have several columns with the `object` data type.  This data type is commonly used as a catch-all and seeing so many columns with this data type should raise a red flag.  \n",
    "\n",
    "If we're going to change data types, we should know what our options are.  Here’s an overview of the main data types you might encounter in a DataFrame.  There are a few more, but these are the most common.\n",
    "\n",
    "1. Numeric Types\n",
    "\n",
    "    int64 / int32: Integer data types, used for columns with whole numbers.\n",
    "    Example: 1, 10, -42\n",
    "    float64 / float32: Floating-point data types, used for columns with decimal numbers.\n",
    "    Example: 3.14, -0.001, 2.5\n",
    "\n",
    "2. Object Type\n",
    "\n",
    "    object: A catch-all data type, typically used for columns that contain strings or mixed data types.\n",
    "    Example: 'apple', '123 Main St', 'true'\n",
    "    Note: This type can include any Python object and is commonly used when data contains text or heterogeneous types. It is generally less efficient than specific types like string.\n",
    "\n",
    "3. String Type\n",
    "\n",
    "    string: Explicit type for columns with text data. While object can hold strings, using string ensures more consistent handling.\n",
    "    Example: 'Hello, world!', 'data science'\n",
    "    Note: This type is useful for text processing and can be more optimized than using object for string data.\n",
    "\n",
    "4. Boolean Type\n",
    "\n",
    "    bool: Represents columns with True or False values.\n",
    "    Example: True, False, True\n",
    "    Use Case: Commonly used in conditions, filters, and logical operations.\n",
    "\n",
    "5. Datetime Type\n",
    "\n",
    "    datetime64: Used for columns containing date and time information.\n",
    "    Example: 2023-11-04 15:45:00, 2024-01-01\n",
    "    Use Case: Facilitates operations like date arithmetic, filtering by dates, and time-series analysis.\n",
    "\n",
    "6. Timedelta Type\n",
    "\n",
    "    timedelta64: Represents differences or durations between dates and times.\n",
    "    Example: 3 days 00:00:00, -1 days +23:00:00\n",
    "    Use Case: Useful for measuring time spans or differences between date columns.\n",
    "\n",
    "7. Category Type\n",
    "\n",
    "    category: Represents data that is limited to a fixed number of distinct values, similar to enumerations.\n",
    "    Example: ['Red', 'Blue', 'Green']\n",
    "    Use Case: Optimizes memory usage and performance when working with columns that have repeated discrete values, like status labels or ratings.\n",
    "    Advantage: Reduces memory usage compared to object and speeds up operations involving comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration, doing some counting...\n",
    "\n",
    "So. We've got many columns in our data frame with the data type `object.`  Many of these seem like the data would be better classified as the `category` data type.  `Category` data has a fixed number of possible values.   `marital.status`, `race`, `sex` seem like they would be good candidates for the `category` data type.  `marital.status` might contain values like \"Married,\" \"Single,\" \"Divorced,\" etc. `race` could include values like \"White,\" \"Black,\" \"Asian,\" etc. `sex` typically has values like \"Male\" and \"Female.\"\n",
    "\n",
    "One way to check if these fields are a good candidate for the `category` data type is to check for the number of unique values.  `Pandas` has a method specifically for this aspect of EDA, `nunique()`.  \n",
    "\n",
    "Run the code cell below to get a count of unique values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultCensus.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we were right about the `marital.status`, `race`, and `sex` columns in our dataframe.  Out of ~32k rows, there are less that 10 unique values in all three columns.  These are defintely good candidates for the `category` data type.  \n",
    "\n",
    "However! We've also discovered that `workclass`, `education`, `occupation`, `relationship`, and `income`have very few unique values as well.  `income` is the one that is really surprising here.  Do you agree?  Why are there only 2 unique values?\n",
    "\n",
    "Let's use the `head()` method from `pandas` again to take another look at the data.  By default, it shows the top 5 rows, but you can specify the number of rows you’d like to see by passing an integer as an argument to head().  The code cell below has the argument 30 in the parentheses, so we will see the firsrt 30 rows.  \n",
    "\n",
    "Run the code cell below.  What do you see in the `income` field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultCensus.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `income` field is just an indicator that tells us whether or not the iondividual makes more than or less than fifty thousand dollars.  So this field is also a perfect candidate for the `category` datatype. \n",
    "\n",
    "Before we convert these fields of our dataframe to the new datatype, let's review why this is important.\n",
    "\n",
    "- Memory Efficiency: The category type stores unique values only once and references them with an integer, which is more efficient than storing strings. This reduces memory usage, which is especially beneficial for large datasets with repeated values in these columns.\n",
    "\n",
    "- Performance Improvement: Operations like grouping, filtering, or comparisons can be faster with categorical data because pandas can optimize them based on the limited number of unique values.\n",
    "\n",
    "- Logical Representation: By converting these columns to category types, you indicate that these fields represent qualitative, not quantitative, information. This makes the code more readable and the data structure clearer.\n",
    "\n",
    "Okay!  Let's look at the `.astype()` method from `pandas`and see how we can use it to change the data type.\n",
    "\n",
    "`adultCensus['marital.status'] = adultCensus['marital.status'].astype('category')`\n",
    "\n",
    "Explanation:\n",
    "1.` adultCensus['marital.status']`: Accesses the `'marital.status'` column of the DataFrame `'adultCensus'`.\n",
    "    - This column currently has a data type of 'object' (typically for text or string values).\n",
    "\n",
    "2. `.astype('category')`: Converts the column's data type to `'category'`.\n",
    "    - The `'category'` data type is memory-efficient, storing each unique value only once\n",
    "and referencing them as integer codes, rather than repeating the entire text string.\n",
    "    - This is particularly useful for columns with a fixed set of repeated values, like\n",
    "marital status categories (\"Married,\" \"Single,\" etc.).\n",
    "\n",
    "3. `adultCensus['marital.status']` = : Assigns the converted column back to `'adultCensus'`, replacing the original version.\n",
    "    - After this line, `'marital.status'` in `'adultCensus'` will have the `'category'` type, reducing memory usage\n",
    "and potentially improving performance for operations like filtering and grouping.\n",
    "\n",
    "Okay!  Now let's run the code and see what happens. Run the `adultCensus.dtypes` code in the following cell as well to see the new data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultCensus['marital.status'] = adultCensus['marital.status'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data type of each column in the adultCensus dataframe \n",
    "adultCensus.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several other columns that we want to convert to the `category` data type as well.  We can use a similar line of code to convert several columns at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultCensus[['income', 'race', 'sex']] = adultCensus[['income', 'race', 'sex']].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data type of each column in the adultCensus dataframe \n",
    "adultCensus.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also \n",
    "\n",
    "Define the threshold for the maximum number of unique values for conversion\n",
    "\n",
    "`threshold = 10  # You can adjust this number as needed`\n",
    "\n",
    "Loop through each column in the DataFrame\n",
    "```\n",
    "for col in adultCensus.columns:\n",
    "    # Check if the number of unique values in the column is less than or equal to the threshold\n",
    "    if adultCensus[col].nunique() <= threshold:\n",
    "        # Convert the column to 'category' type\n",
    "        adultCensus[col] = adultCensus[col].astype('category')\n",
    "```\n",
    "\n",
    "Verify the changes\n",
    "`adultCensus.dtypes`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold for the maximum number of unique values for conversion\n",
    "threshold = 10  # You can adjust this number as needed\n",
    "\n",
    "# Loop through each column in the DataFrame\n",
    "for col in adultCensus.columns:\n",
    "    # Check if the number of unique values in the column is less than or equal to the threshold\n",
    "    if adultCensus[col].nunique() <= threshold:\n",
    "        # Convert the column to 'category' type\n",
    "        adultCensus[col] = adultCensus[col].astype('category')\n",
    "\n",
    "# Verify the changes\n",
    "adultCensus.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object\n",
    "\n",
    "Now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
