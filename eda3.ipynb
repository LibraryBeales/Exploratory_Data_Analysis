{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email [rdb104@case.edu](mailto:rdb104@case.edu).<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in Python\n",
    "\n",
    "**Description:** This lesson introduces the basic data import and simple assessment processes using the `pandas` library for Python.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 15 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** csv files\n",
    "\n",
    "**Data Format:** `csv`, `py` \n",
    "\n",
    "**Libraries Used:** `pandas` \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Welcome to your first EDA lesson.  This project will use the `pandas` package to introduce the basic EDA workflow.\n",
    "\n",
    "We will be using [2014 Adult Census Data](https://raw.githubusercontent.com/LibraryBeales/Exploratory_Data_Analysis/refs/heads/main/adult.csv) as our example for this tutorial.  The csv file is taken from this [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/adult-census-income?resource=download).  \n",
    "\n",
    "## Project #3: Identify missing data and decide how to handle them.\n",
    "\n",
    "Build a scraper that collects multiple data points about each book based upon specific criteria.\n",
    "\n",
    "In this project you will:\n",
    "1. Determine what data we are able to collect about each book listed in the store.  \n",
    "2. Use the `Inspect` tool in your web browser to identify the web page structure for those pieces of data. \n",
    "3. Understand and use a python script to crawl the web page and extract only the data that meets the classification criteria we identified in steps 1 and 2.\n",
    "4. Write the data to a csv file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dsfasdfasdfasd"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
